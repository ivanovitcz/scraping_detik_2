{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen as uReq\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import re\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cek paginasi, hanya ambil 2 page, karena pada beberapa kategori berita di Detik tidak ada pagination nya \n",
    "\n",
    "def check_pagination(jenis, date):\n",
    "    all_url = []\n",
    "    for i in range(1,5):\n",
    "        check_url = 0\n",
    "        url_by_page = \"https://{}.detik.com/indeks/{}?date={}\".format(jenis,i,date)\n",
    "        url_c = uReq(url_by_page)\n",
    "        html_page = url_c.read()\n",
    "        url_c.close()\n",
    "        page_html_soup = soup(html_page, \"html.parser\")\n",
    "        \n",
    "        if(jenis == \"travel\"):\n",
    "            check_content = page_html_soup.select(\"article\")\n",
    "        else:\n",
    "            check_content = page_html_soup.select(\"ul li article\")\n",
    "            \n",
    "        if(len(check_content) != 0):\n",
    "            all_url.append(url_by_page)\n",
    "        \n",
    "    return all_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping content pada satu page\n",
    "\n",
    "def one_page_content(jenis, container):   \n",
    "    berita = []\n",
    "\n",
    "    for article in container:\n",
    "        temp_list = []\n",
    "        \n",
    "        if(jenis == \"travel\"):\n",
    "            url = article.select(\"h3 a\")[0].attrs[\"href\"]\n",
    "            title = article.select(\"h3 a\")[0].text\n",
    "        else:\n",
    "            url = article.select(\"a\")[0].attrs[\"href\"]\n",
    "            title = article.select(\"h2\")[0].text\n",
    "\n",
    "\n",
    "        url_content  = uReq(url+\"?single=1\")\n",
    "        try:\n",
    "            html_content = url_content.read()\n",
    "        except SocketError as e:\n",
    "            if e.errno != errno.ECONNRESET:\n",
    "                raise \n",
    "            pass\n",
    "        url_content.close()\n",
    "        \n",
    "        content_soup = soup(html_content, \"html.parser\")\n",
    "        \n",
    "        if(jenis == \"travel\"):\n",
    "            container_content = content_soup.select(\"div.content p\")  \n",
    "        elif(jenis == \"food\"):\n",
    "            container_content = content_soup.select(\"article p\")  \n",
    "        else:\n",
    "            container_content = content_soup.findAll(\"p\")  \n",
    "            \n",
    "        \n",
    "        content = \"\"\n",
    "        print(title)\n",
    "        if(len(container_content) == 0):\n",
    "            if(jenis == \"food\"):\n",
    "                content = content_soup.select(\"div.newstag span\")\n",
    "                if(len(content) == 0):\n",
    "                    content = content_soup.select(\"div.detail_text p\")\n",
    "                    content = content[0].text\n",
    "                    content = content.replace(\"\\n\",\"\")\n",
    "                    content = content.replace(\"\\t\",\"\")\n",
    "                else:\n",
    "                    content = content[0].text\n",
    "            else:\n",
    "                content = content_soup.findAll(\"div\",{\"class\":\"detail_text\"})\n",
    "                content = content[0].text\n",
    "                content = content.replace(\"\\n\",\"\")\n",
    "                content = content.replace(\"\\t\",\"\")\n",
    "        else:\n",
    "            for i in container_content:\n",
    "                content = content + \" \" + i.text\n",
    "\n",
    "        temp_list.extend([url,title,content])\n",
    "\n",
    "        berita.append(temp_list)\n",
    "        \n",
    "    berita = pd.DataFrame(berita, columns = ['url','title','content'])        \n",
    "    return berita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#melakukan scraping pada tanggal spesifik\n",
    "#sesuaikan bulan date, format mm/dd/yyyy\n",
    "#sesuaikan jenis kategori berita yang diinginkan\n",
    "\n",
    "def semua(tanggal):\n",
    "    #sesuaikan date, format mm/dd/yyyy\n",
    "    date = \"10/\"+str(tanggal)+\"/2020\"\n",
    "    #jenis berita Detik (news, finance, hot)\n",
    "    jenis = \"food\"\n",
    "\n",
    "\n",
    "    df = pd.DataFrame([], columns=['url','title','content'])\n",
    "    pagination = check_pagination(jenis, date)\n",
    "    print(\"total page \"+str(len(pagination)))\n",
    "\n",
    "    count = 0\n",
    "    for i in pagination:\n",
    "        count += 1\n",
    "        print(\"page\", count)\n",
    "        page_url = uReq(i)\n",
    "        html_page = page_url.read()\n",
    "        page_url.close()\n",
    "        html_soup_page = soup(html_page, \"html.parser\")\n",
    "        \n",
    "        if((jenis == \"oto\") or (jenis == \"sport\")):\n",
    "            container_page = html_soup_page.select(\"ul li article div.desc_idx\")\n",
    "        elif(jenis == \"travel\"):\n",
    "            container_page = html_soup_page.select(\"article\")\n",
    "        else:\n",
    "            #health\n",
    "            container_page = html_soup_page.select(\"ul li article\")\n",
    "            \n",
    "        berita = one_page_content(jenis, container_page)\n",
    "        df = df.append(berita, ignore_index=True)\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++++++==========MULAI TANGGAL 31=============================\n",
      "total page 2\n",
      "page 1\n",
      "Ngeri Sedap! Pesta Ulang Tahun Ini Dipenuhi Kue Darah hingga Tengkorak\n",
      "Paula Verhoeven Hamil Anak Kedua, Sosoknya Menawan Saat Kulineran\n",
      "5 Makanan Mematikan yang Tetap Populer Meski Berbahaya\n",
      "Apakah Bisa MSG Menggantikan Garam? Ini Penjelasan Ahli\n",
      "Mirip Situs Porno dan Alat Kelamin, Nama 5 Restoran Ini Bikin Salah Fokus\n",
      "5 Makanan Khas Turki yang Terkenal Enak, Bikin Nagih!\n",
      "Kulineran Murah Warung Nasi Sunda Kaki Lima di Jalan Ciheuleut\n",
      "Wow! Penemuan 5 Ikan Kerapu Terbesar di Dunia Ini Bikin Heboh\n",
      "Wow! Ada Es Krim Rasa Pecel dan Rujak, Gimana Rasanya?\n",
      "Hobi Bikin Kue? Ini Alat-alat Baking yang Perlu Kamu Miliki\n",
      "5 Tempat Makan Enak di TMII, Tawarkan Pecel Madiun hingga Patin Bakar\n",
      "Makan Nasi Goreng dan Es Teh Buatan Hotel, Netizen Ini Habiskan Rp 2 Juta!\n",
      "Kulineran ke Bogor Permai, Ada Toge Goreng hingga Es Sekoteng Menanti\n",
      "Kepincut Enaknya Toge Goreng Hj. Omah Favoritnya Presiden Jokowi\n",
      "Pecel Legendaris Mbok Pakem Tetap Ramai Meski Berlokasi Dekat Kuburan\n",
      "Medja Restaurant Bogor, Resto Keluarga Bergaya Rustic Nan Asri\n",
      "Modal Rp 50 Ribu Bisa Jajan Kenyang di Jalan Suryakencana\n",
      "Dokter Ini Ungkap Kalori Es Kopi Susu Gula Aren, Bikin Melongo!\n",
      "Pedas Manis Toge Goreng Saus Tauco Hj Omah\n",
      "Bikin Laper! Pedas Segar Ikan Tude Dabu-dabu dan Ayam Tuturuga\n",
      "page 2\n",
      "7 Jus untuk Kolesterol yang Baik untuk Menjaga Tekanan Darah Tetap Stabil\n",
      "Tempat Makan Langganan Jokowi hingga Nama Warung Nyleneh\n",
      "Cukup 3 Bahan Bisa Bikin Roti Panggang Keju dan Bawang yang Renyah\n",
      "Minum Teh Bikin Kurus? Ini 7 Kebiasaan Minum yang Cocok untuk Diet\n",
      "5 Cara Membuat Minuman Lemon untuk Kesehatan yang Enak\n",
      "\n",
      "Percent of missing url records is 0.00%\n",
      "Percent of missing title records is 0.00%\n",
      "Percent of missing content records is 0.00%\n",
      "\n",
      "+++++++++==========DONE TANGGAL 31=============================\n"
     ]
    }
   ],
   "source": [
    "#looping pada range tanggal tertentu\n",
    "#sesuaikan bulan date, format mm/dd/yyyy\n",
    "#sesuaikan jenis kategori berita yang diinginkan\n",
    "\n",
    "for i in range(31,32):\n",
    "    print(\"+++++++++==========MULAI TANGGAL \"+str(i)+\"=============================\")    \n",
    "    result = semua(i)\n",
    "    columns = result.columns.values\n",
    "    print()\n",
    "    for j in columns:\n",
    "        missing = (result[j].isnull().sum()/result.shape[0])*100\n",
    "        print('Percent of missing '+j+' records is %.2f%%' %(missing))\n",
    "    print()\n",
    "    jenis_result = \"food\"\n",
    "    date_result = \"10/\"+str(i)+\"/2020\"\n",
    "    date_format = date_result.split(\"/\")\n",
    "    result.to_csv(\"detik-\"+jenis_result+\"-\"+date_format[1]+\"-\"+date_format[0]+\"-\"+date_format[2]+\".csv\")\n",
    "    print(\"+++++++++==========DONE TANGGAL \"+str(i)+\"=============================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
